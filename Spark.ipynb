{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix A:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Pseudoinverse of A:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n",
      "\n",
      "Inverse of A:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "def pseudoinverse(A):\n",
    "    \"\"\"Die Funktion Berechnet die Pseudoinverse Mittels SVD\"\"\"\n",
    "    # Perform Singular Value Decomposition\n",
    "    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Compute the reciprocal of non-zero singular values\n",
    "    S_inv = np.diag(1.0 / S)\n",
    "    \n",
    "    # Compute pseudoinverse using Vt.T, S_inv, and U.T\n",
    "    A_pseudo = Vt.T @ S_inv @ U.T\n",
    "    \n",
    "    return A_pseudo\n",
    "\n",
    "# Example matrix (can be non-square or singular)\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Calculate the pseudoinverse\n",
    "A_pseudo = pseudoinverse(A)\n",
    "inv_A_real = np.linalg.inv(A)\n",
    "\n",
    "print(\"Original Matrix A:\\n\", A)\n",
    "print(\"\\nPseudoinverse of A:\\n\", A_pseudo)\n",
    "print(\"\\nInverse of A:\\n\", inv_A_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressors (β): [1.]\n"
     ]
    }
   ],
   "source": [
    "def calculate_regressors(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the regression coefficients (β) using the Moore-Penrose pseudoinverse.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): The feature matrix (m x n)\n",
    "    y (numpy array): The target vector (m x 1)\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: The regression coefficients β (n x 1)\n",
    "    \"\"\"\n",
    "    # Compute the Moore-Penrose pseudoinverse of X\n",
    "    X_pseudo = pseudoinverse(X)\n",
    "    \n",
    "    # Calculate the regression coefficients\n",
    "    beta = X_pseudo @ y\n",
    "    \n",
    "    return beta\n",
    "\n",
    "# Example usage:\n",
    "# Feature matrix X (with a column of ones for the intercept)\n",
    "X = np.array([[1], [2], [3], [4],[5]])\n",
    "\n",
    "# Target values y\n",
    "y = np.array([1,2,3,4,5])\n",
    "\n",
    "# Calculate the regression coefficients β\n",
    "beta = calculate_regressors(X, y)\n",
    "\n",
    "print(\"Regressors (β):\", beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:               0.9797\n",
      "Model:                            OLS   Adj. R-squared:          0.9695\n",
      "Method:                 Least Squares   F-statistic:              96.3333\n",
      "Date:                Wed, 09 Oct 2024   Prob (F-statistic):        1.0222e-02\n",
      "Time:                        20:02:35   Log-Likelihood:           -0.4952\n",
      "No. Observations:                 4   AIC:                    4.9904\n",
      "Df Residuals:                    2   BIC:                    3.7630\n",
      "Df Model:                        1\n",
      "Covariance Type:            nonrobust\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "          const     0.0000     0.4743     0.0000     1.0000 [   -2.0409,     2.0409]\n",
      "             x1     1.7000     0.1732     9.8150     0.0102 [    0.9548,     2.4452]\n",
      "==============================================================================\n",
      "Omnibus:                   nan   Durbin-Watson:           2.2333\n",
      "Prob(Omnibus):             nan   Jarque-Bera (JB):        0.4281\n",
      "Skew:                      -0.3651   Prob(JB):                0.8073\n",
      "Kurtosis:                  1.5733   Cond. No.             7.4687\n",
      "==============================================================================\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe\\AppData\\Local\\Temp\\ipykernel_28560\\3264700245.py:86: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  Omnibus_stat, Omnibus_p_value = stats.normaltest(residuals_flat)\n"
     ]
    }
   ],
   "source": [
    "#Implementierung des statsmodels (Formatierung muss noch angepasst werden, dass es mit dem unteren stimmt)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "def linear_regression_statsmodels_like(X, y):\n",
    "    \"\"\"\n",
    "    Führt eine lineare Regression durch und gibt eine Zusammenfassung aus,\n",
    "    die der OLS Regression Results von statsmodels entspricht.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): Feature-Matrix (m x n), ohne Intercept-Spalte.\n",
    "    y (numpy array): Zielvektor (m x 1).\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Sicherstellen, dass y ein Spaltenvektor ist\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    # Anzahl der Beobachtungen und Prädiktoren\n",
    "    n = X.shape[0]  # Anzahl der Beobachtungen\n",
    "    k = X.shape[1]  # Anzahl der Prädiktoren (ohne Intercept)\n",
    "    p = k + 1       # Anzahl der Parameter (inklusive Intercept)\n",
    "    \n",
    "    # Intercept-Spalte zu X hinzufügen\n",
    "    X = np.hstack((np.ones((n, 1)), X))\n",
    "    \n",
    "    # Koeffizienten mit der Normalengleichung berechnen\n",
    "    XtX_inv = np.linalg.inv(X.T @ X)\n",
    "    beta_hat = XtX_inv @ X.T @ y  # (p x 1)\n",
    "    \n",
    "    # Vorhergesagte Werte\n",
    "    y_hat = X @ beta_hat  # (n x 1)\n",
    "    \n",
    "    # Residuen\n",
    "    residuals = y - y_hat  # (n x 1)\n",
    "    residuals_flat = residuals.flatten()\n",
    "    \n",
    "    # Freiheitsgrade\n",
    "    df_model = p - 1       # Modell-Freiheitsgrade\n",
    "    df_resid = n - p       # Residuen-Freiheitsgrade\n",
    "    \n",
    "    # Sum of Squares\n",
    "    y_mean = np.mean(y)\n",
    "    TSS = np.sum((y - y_mean) ** 2)   # Total Sum of Squares\n",
    "    RSS = np.sum(residuals_flat ** 2) # Residual Sum of Squares\n",
    "    ESS = TSS - RSS                   # Explained Sum of Squares\n",
    "    \n",
    "    # Varianz und Standardfehler\n",
    "    sigma2 = RSS / df_resid\n",
    "    var_beta_hat = sigma2 * XtX_inv\n",
    "    se_beta_hat = np.sqrt(np.diag(var_beta_hat)).reshape(-1, 1)\n",
    "    \n",
    "    # t-Werte und p-Werte für die Koeffizienten\n",
    "    t_stats = beta_hat.flatten() / se_beta_hat.flatten()\n",
    "    p_values = [2 * (1 - stats.t.cdf(np.abs(t), df_resid)) for t in t_stats]\n",
    "    \n",
    "    # R-Quadrat und Adjusted R-Quadrat\n",
    "    R_squared = 1 - RSS / TSS\n",
    "    adj_R_squared = 1 - (1 - R_squared) * (n - 1) / df_resid\n",
    "    \n",
    "    # Mittlere Quadratsummen\n",
    "    MSR = ESS / df_model\n",
    "    MSE = RSS / df_resid\n",
    "    \n",
    "    # F-Statistik und p-Wert\n",
    "    F_stat = MSR / MSE\n",
    "    F_p_value = 1 - stats.f.cdf(F_stat, df_model, df_resid)\n",
    "    \n",
    "    # Log-Likelihood\n",
    "    LLF = - (n / 2) * (np.log(2 * np.pi) + np.log(RSS / n) + 1)\n",
    "    \n",
    "    # AIC und BIC\n",
    "    AIC = -2 * LLF + 2 * p\n",
    "    BIC = -2 * LLF + p * np.log(n)\n",
    "    \n",
    "    # Durbin-Watson-Statistik\n",
    "    diff_resid = np.diff(residuals_flat, n=1)\n",
    "    DW_stat = np.sum(diff_resid ** 2) / np.sum(residuals_flat ** 2)\n",
    "    \n",
    "    # Skewness und Kurtosis der Residuen\n",
    "    skewness = stats.skew(residuals_flat)\n",
    "    kurtosis = stats.kurtosis(residuals_flat, fisher=False)  # Pearson's Definition\n",
    "    \n",
    "    # Omnibus-Test\n",
    "    Omnibus_stat, Omnibus_p_value = stats.normaltest(residuals_flat)\n",
    "    \n",
    "    # Jarque-Bera-Test\n",
    "    JB_stat, JB_p_value = stats.jarque_bera(residuals_flat)\n",
    "    \n",
    "    # Bedingungsnummer\n",
    "    _, svals, _ = np.linalg.svd(X)\n",
    "    cond_no = svals[0] / svals[-1]\n",
    "    \n",
    "    # Konfidenzintervalle\n",
    "    alpha = 0.05\n",
    "    t_crit = stats.t.ppf(1 - alpha / 2, df_resid)\n",
    "    ci_lower = beta_hat.flatten() - t_crit * se_beta_hat.flatten()\n",
    "    ci_upper = beta_hat.flatten() + t_crit * se_beta_hat.flatten()\n",
    "    \n",
    "    # Aktuelles Datum und Uhrzeit\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime(\"%a, %d %b %Y\")\n",
    "    time_str = now.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    # Ausgabe ähnlich wie statsmodels\n",
    "    print(\"                            OLS Regression Results                            \")\n",
    "    print(\"==============================================================================\")\n",
    "    print(f\"Dep. Variable:                      y   R-squared:               {R_squared:.4f}\")\n",
    "    print(f\"Model:                            OLS   Adj. R-squared:          {adj_R_squared:.4f}\")\n",
    "    print(f\"Method:                 Least Squares   F-statistic:              {F_stat:.4f}\")\n",
    "    print(f\"Date:                {date_str}   Prob (F-statistic):        {F_p_value:.4e}\")\n",
    "    print(f\"Time:                        {time_str}   Log-Likelihood:           {LLF:.4f}\")\n",
    "    print(f\"No. Observations:                 {n}   AIC:                    {AIC:.4f}\")\n",
    "    print(f\"Df Residuals:                    {df_resid}   BIC:                    {BIC:.4f}\")\n",
    "    print(f\"Df Model:                        {df_model}\")\n",
    "    print(\"Covariance Type:            nonrobust\")\n",
    "    print(\"==============================================================================\")\n",
    "    print(\"                 coef    std err          t      P>|t|      [0.025      0.975]\")\n",
    "    \n",
    "    # Variablennamen\n",
    "    var_names = ['const'] + [f'x{i}' for i in range(1, p)]\n",
    "    \n",
    "    for i in range(p):\n",
    "        print(f\"{var_names[i]:>15} {beta_hat[i,0]:>10.4f} {se_beta_hat[i,0]:>10.4f} {t_stats[i]:>10.4f} {p_values[i]:>10.4f} [{ci_lower[i]:>10.4f}, {ci_upper[i]:>10.4f}]\")\n",
    "    \n",
    "    print(\"==============================================================================\")\n",
    "    print(f\"Omnibus:                   {Omnibus_stat:.4f}   Durbin-Watson:           {DW_stat:.4f}\")\n",
    "    print(f\"Prob(Omnibus):             {Omnibus_p_value:.4f}   Jarque-Bera (JB):        {JB_stat:.4f}\")\n",
    "    print(f\"Skew:                      {skewness:.4f}   Prob(JB):                {JB_p_value:.4f}\")\n",
    "    print(f\"Kurtosis:                  {kurtosis:.4f}   Cond. No.             {cond_no:.4f}\")\n",
    "    print(\"==============================================================================\")\n",
    "    print(\"Notes:\")\n",
    "    print(\"[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\")\n",
    "\n",
    "# Beispielverwendung:\n",
    "\n",
    "# Feature-Matrix X (ohne Intercept-Spalte)\n",
    "X = np.array([[1], [2], [3], [4]])  # (n x k)\n",
    "# Zielvektor y\n",
    "y = np.array([2, 3, 5, 7])          # (n,)\n",
    "\n",
    "# Lineare Regression durchführen und Zusammenfassung ausgeben\n",
    "linear_regression_statsmodels_like(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.980\n",
      "Model:                            OLS   Adj. R-squared:                  0.969\n",
      "Method:                 Least Squares   F-statistic:                     96.33\n",
      "Date:                Wed, 09 Oct 2024   Prob (F-statistic):             0.0102\n",
      "Time:                        20:04:52   Log-Likelihood:               -0.49522\n",
      "No. Observations:                   4   AIC:                             4.990\n",
      "Df Residuals:                       2   BIC:                             3.763\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const               0      0.474          0      1.000      -2.041       2.041\n",
      "x1             1.7000      0.173      9.815      0.010       0.955       2.445\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   2.233\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.428\n",
      "Skew:                          -0.365   Prob(JB):                        0.807\n",
      "Kurtosis:                       1.573   Cond. No.                         7.47\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\stats\\stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    }
   ],
   "source": [
    "#Zum Überprüfen ob Berechnung korrekt ist\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1], [2], [3], [4]])  # Feature matrix\n",
    "y = np.array([2, 3, 5, 7])          # Target values\n",
    "\n",
    "# Add an intercept (constant) to the model\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "\n",
    "# Fit the model using OLS (Ordinary Least Squares)\n",
    "model = sm.OLS(y, X_with_intercept)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
