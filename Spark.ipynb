{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix A:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Pseudoinverse of A:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n",
      "\n",
      "Inverse of A:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "def pseudoinverse(A):\n",
    "    \"\"\"Die Funktion Berechnet die Pseudoinverse Mittels SVD\"\"\"\n",
    "    # Perform Singular Value Decomposition\n",
    "    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Compute the reciprocal of non-zero singular values\n",
    "    S_inv = np.diag(1.0 / S)\n",
    "    \n",
    "    # Compute pseudoinverse using Vt.T, S_inv, and U.T\n",
    "    A_pseudo = Vt.T @ S_inv @ U.T\n",
    "    \n",
    "    return A_pseudo\n",
    "\n",
    "# Example matrix (can be non-square or singular)\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Calculate the pseudoinverse\n",
    "A_pseudo = pseudoinverse(A)\n",
    "inv_A_real = np.linalg.inv(A)\n",
    "\n",
    "print(\"Original Matrix A:\\n\", A)\n",
    "print(\"\\nPseudoinverse of A:\\n\", A_pseudo)\n",
    "print(\"\\nInverse of A:\\n\", inv_A_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressors (β): [1.]\n"
     ]
    }
   ],
   "source": [
    "def calculate_regressors(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the regression coefficients (β) using the Moore-Penrose pseudoinverse.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): The feature matrix (m x n)\n",
    "    y (numpy array): The target vector (m x 1)\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: The regression coefficients β (n x 1)\n",
    "    \"\"\"\n",
    "    # Compute the Moore-Penrose pseudoinverse of X\n",
    "    X_pseudo = pseudoinverse(X)\n",
    "    \n",
    "    # Calculate the regression coefficients\n",
    "    beta = X_pseudo @ y\n",
    "    \n",
    "    return beta\n",
    "\n",
    "# Example usage:\n",
    "# Feature matrix X (with a column of ones for the intercept)\n",
    "X = np.array([[1], [2], [3], [4],[5]])\n",
    "\n",
    "# Target values y\n",
    "y = np.array([1,2,3,4,5])\n",
    "\n",
    "# Calculate the regression coefficients β\n",
    "beta = calculate_regressors(X, y)\n",
    "\n",
    "print(\"Regressors (β):\", beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                      0.8399\n",
      "Model:                            OLS   Adj. R-squared:                 0.8171\n",
      "Method:                 Least Squares   F-statistic:                   36.7318\n",
      "Date:                Thu, 10 Oct 2024   Prob (F-statistic):             0.0005\n",
      "Time:                        13:57:11   Log-Likelihood:               -27.2501\n",
      "No. Observations:                   9   AIC:                           58.5001\n",
      "Df Residuals:                       7   BIC:                           58.8946\n",
      "Df Model:                           1\n",
      "Covariance Type:            nonrobust\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -8.1667     4.1163    -1.9840     0.0877    -17.9002      1.5669\n",
      "x1             4.4333     0.7315     6.0607     0.0005      2.7036      6.1630\n",
      "==============================================================================\n",
      "Omnibus:                       0.1095   Durbin-Watson:                  0.5976\n",
      "Prob(Omnibus):                 0.9467   Jarque-Bera (JB):               0.3236\n",
      "Skew:                          0.0178   Prob(JB):                       0.8506\n",
      "Kurtosis:                      2.0717   Cond. No.                      12.5722\n",
      "==============================================================================\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Implementierung des statsmodels (Formatierung muss noch angepasst werden, dass es mit dem unteren stimmt)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "def linear_regression_statsmodels_like(X, y):\n",
    "    \"\"\"\n",
    "    Führt eine lineare Regression durch und gibt eine Zusammenfassung aus,\n",
    "    die der OLS Regression Results von statsmodels entspricht.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): Feature-Matrix (m x n), ohne Intercept-Spalte.\n",
    "    y (numpy array): Zielvektor (m x 1).\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Sicherstellen, dass y ein Spaltenvektor ist\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    # Anzahl der Beobachtungen und Prädiktoren\n",
    "    n = X.shape[0]  # Anzahl der Beobachtungen\n",
    "    k = X.shape[1]  # Anzahl der Prädiktoren (ohne Intercept)\n",
    "    p = k + 1       # Anzahl der Parameter (inklusive Intercept)\n",
    "    \n",
    "    # Intercept-Spalte zu X hinzufügen\n",
    "    X = np.hstack((np.ones((n, 1)), X))\n",
    "    \n",
    "    # Koeffizienten mit der Normalengleichung berechnen\n",
    "    XtX_inv = np.linalg.inv(X.T @ X)\n",
    "    beta_hat = XtX_inv @ X.T @ y  # (p x 1)\n",
    "    \n",
    "    # Vorhergesagte Werte\n",
    "    y_hat = X @ beta_hat  # (n x 1)\n",
    "    \n",
    "    # Residuen\n",
    "    residuals = y - y_hat  # (n x 1)\n",
    "    residuals_flat = residuals.flatten()\n",
    "    \n",
    "    # Freiheitsgrade\n",
    "    df_model = p - 1       # Modell-Freiheitsgrade\n",
    "    df_resid = n - p       # Residuen-Freiheitsgrade\n",
    "    \n",
    "    # Sum of Squares\n",
    "    y_mean = np.mean(y)\n",
    "    TSS = np.sum((y - y_mean) ** 2)   # Total Sum of Squares\n",
    "    RSS = np.sum(residuals_flat ** 2) # Residual Sum of Squares\n",
    "    ESS = TSS - RSS                   # Explained Sum of Squares\n",
    "    \n",
    "    # Varianz und Standardfehler\n",
    "    sigma2 = RSS / df_resid\n",
    "    var_beta_hat = sigma2 * XtX_inv\n",
    "    se_beta_hat = np.sqrt(np.diag(var_beta_hat)).reshape(-1, 1)\n",
    "    \n",
    "    # t-Werte und p-Werte für die Koeffizienten\n",
    "    t_stats = beta_hat.flatten() / se_beta_hat.flatten()\n",
    "    p_values = [2 * (1 - stats.t.cdf(np.abs(t), df_resid)) for t in t_stats]\n",
    "    \n",
    "    # R-Quadrat und Adjusted R-Quadrat\n",
    "    R_squared = 1 - RSS / TSS\n",
    "    adj_R_squared = 1 - (1 - R_squared) * (n - 1) / df_resid\n",
    "    \n",
    "    # Mittlere Quadratsummen\n",
    "    MSR = ESS / df_model\n",
    "    MSE = RSS / df_resid\n",
    "    \n",
    "    # F-Statistik und p-Wert\n",
    "    F_stat = MSR / MSE\n",
    "    F_p_value = 1 - stats.f.cdf(F_stat, df_model, df_resid)\n",
    "    \n",
    "    # Log-Likelihood\n",
    "    LLF = - (n / 2) * (np.log(2 * np.pi) + np.log(RSS / n) + 1)\n",
    "    \n",
    "    # AIC und BIC\n",
    "    AIC = -2 * LLF + 2 * p\n",
    "    BIC = -2 * LLF + p * np.log(n)\n",
    "    \n",
    "    # Durbin-Watson-Statistik\n",
    "    diff_resid = np.diff(residuals_flat, n=1)\n",
    "    DW_stat = np.sum(diff_resid ** 2) / np.sum(residuals_flat ** 2)\n",
    "    \n",
    "    # Skewness und Kurtosis der Residuen\n",
    "    skewness = stats.skew(residuals_flat)\n",
    "    kurtosis = stats.kurtosis(residuals_flat, fisher=False)  # Pearson's Definition\n",
    "    \n",
    "    # Omnibus-Test\n",
    "    Omnibus_stat, Omnibus_p_value = stats.normaltest(residuals_flat)\n",
    "    \n",
    "    # Jarque-Bera-Test\n",
    "    JB_stat, JB_p_value = stats.jarque_bera(residuals_flat)\n",
    "    \n",
    "    # Bedingungsnummer\n",
    "    _, svals, _ = np.linalg.svd(X)\n",
    "    cond_no = svals[0] / svals[-1]\n",
    "    \n",
    "    # Konfidenzintervalle\n",
    "    alpha = 0.05\n",
    "    t_crit = stats.t.ppf(1 - alpha / 2, df_resid)\n",
    "    ci_lower = beta_hat.flatten() - t_crit * se_beta_hat.flatten()\n",
    "    ci_upper = beta_hat.flatten() + t_crit * se_beta_hat.flatten()\n",
    "    \n",
    "    # Aktuelles Datum und Uhrzeit\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime(\"%a, %d %b %Y\")\n",
    "    time_str = now.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    # Ausgabe ähnlich wie statsmodels\n",
    "    print(\"                            OLS Regression Results                            \")\n",
    "    print(\"==============================================================================\")\n",
    "    print(f\"Dep. Variable:                      y   R-squared:                  {R_squared:>10.4f}\")\n",
    "    print(f\"Model:                            OLS   Adj. R-squared:             {adj_R_squared:>10.4f}\")\n",
    "    print(f\"Method:                 Least Squares   F-statistic:                {F_stat:>10.4f}\")\n",
    "    print(f\"Date:                {date_str}   Prob (F-statistic):         {F_p_value:>10.4f}\")\n",
    "    print(f\"Time:                        {time_str}   Log-Likelihood:             {LLF:>10.4f}\")\n",
    "    print(f\"No. Observations:          {n:>10}   AIC:                        {AIC:>10.4f}\")\n",
    "    print(f\"Df Residuals:              {df_resid:>10}   BIC:                        {BIC:>10.4f}\")\n",
    "    print(f\"Df Model:                  {df_model:>10}\")\n",
    "    print(\"Covariance Type:            nonrobust\")\n",
    "    print(\"==============================================================================\")\n",
    "    print(\"                 coef    std err          t      P>|t|      [0.025      0.975]\")\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    # Variablennamen\n",
    "    var_names = ['const'] + [f'x{i}' for i in range(1, p)]\n",
    "    \n",
    "    for i in range(p):\n",
    "        print(f\"{var_names[i]:<10} {beta_hat[i,0]:>10.4f} {se_beta_hat[i,0]:>10.4f} {t_stats[i]:>10.4f} {p_values[i]:>10.4f} {ci_lower[i]:>11.4f} {ci_upper[i]:>11.4f}\")\n",
    "    \n",
    "    print(\"==============================================================================\")\n",
    "    print(f\"Omnibus:                   {Omnibus_stat:>10.4f}   Durbin-Watson:              {DW_stat:>10.4f}\")\n",
    "    print(f\"Prob(Omnibus):             {Omnibus_p_value:>10.4f}   Jarque-Bera (JB):           {JB_stat:>10.4f}\")\n",
    "    print(f\"Skew:                      {skewness:>10.4f}   Prob(JB):                   {JB_p_value:>10.4f}\")\n",
    "    print(f\"Kurtosis:                  {kurtosis:>10.4f}   Cond. No.                   {cond_no:>10.4f}\")\n",
    "    print(\"==============================================================================\")\n",
    "    print(\"Notes:\")\n",
    "    print(\"[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\")\n",
    "\n",
    "# Beispielverwendung:\n",
    "\n",
    "# Feature-Matrix X (ohne Intercept-Spalte)\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # (n x k)\n",
    "# Zielvektor y\n",
    "y = np.array([2, 3, 5, 7, 9, 10, 20, 30, 40])          # (n,)\n",
    "\n",
    "# Lineare Regression durchführen und Zusammenfassung ausgeben\n",
    "linear_regression_statsmodels_like(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.840\n",
      "Model:                            OLS   Adj. R-squared:                  0.817\n",
      "Method:                 Least Squares   F-statistic:                     36.73\n",
      "Date:                Thu, 10 Oct 2024   Prob (F-statistic):           0.000511\n",
      "Time:                        12:56:06   Log-Likelihood:                -27.250\n",
      "No. Observations:                   9   AIC:                             58.50\n",
      "Df Residuals:                       7   BIC:                             58.89\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -8.1667      4.116     -1.984      0.088     -17.900       1.567\n",
      "x1             4.4333      0.731      6.061      0.001       2.704       6.163\n",
      "==============================================================================\n",
      "Omnibus:                        0.110   Durbin-Watson:                   0.598\n",
      "Prob(Omnibus):                  0.947   Jarque-Bera (JB):                0.324\n",
      "Skew:                           0.018   Prob(JB):                        0.851\n",
      "Kurtosis:                       2.072   Cond. No.                         12.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john-\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1736: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=9\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    }
   ],
   "source": [
    "#Zum Überprüfen ob Berechnung korrekt ist\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Feature-Matrix X (ohne Intercept-Spalte)\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # (n x k)\n",
    "# Zielvektor y\n",
    "y = np.array([2, 3, 5, 7, 9, 10, 20, 30, 40])          # (n,)\n",
    "\n",
    "# Add an intercept (constant) to the model\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "\n",
    "# Fit the model using OLS (Ordinary Least Squares)\n",
    "model = sm.OLS(y, X_with_intercept)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
