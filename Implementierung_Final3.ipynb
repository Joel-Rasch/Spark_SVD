{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c814a0-3899-4c93-993f-b13061cda8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final CODE PARALLEL\n",
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def create_block_matrix_from_numpy(\n",
    "    np_array: np.ndarray,\n",
    "    row_block_count: int,\n",
    "    col_block_count: int,\n",
    "    sc\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create a BlockMatrix from a NumPy array.\n",
    "    \n",
    "    Args:\n",
    "        np_array (np.ndarray): NumPy array to be converted\n",
    "        row_block_count (int): Number of rows in each block\n",
    "        col_block_count (int): Number of columns in each block\n",
    "        sc: SparkContext for parallelization\n",
    "        \n",
    "    Returns:\n",
    "        BlockMatrix: Distributed block matrix representation of input array\n",
    "    \"\"\"\n",
    "    num_rows: int = 0\n",
    "    num_cols: int = 0\n",
    "    blocks: list = []\n",
    "    blocks_rdd: pyspark.RDD = None\n",
    "    block_dense_matrix: Matrices.dense = None\n",
    "    \n",
    "    num_rows, num_cols = np_array.shape\n",
    "\n",
    "    for j in range(0, num_cols, col_block_count):\n",
    "        for i in range(0, num_rows, row_block_count):\n",
    "            block = np_array[i:i + row_block_count, j:j + col_block_count]\n",
    "            block_dense_matrix = Matrices.dense(block.shape[0], block.shape[1], block.T.flatten())\n",
    "            blocks.append(((i // row_block_count, j // col_block_count), block_dense_matrix))\n",
    "\n",
    "    blocks_rdd = sc.parallelize(blocks)\n",
    "    \n",
    "    return BlockMatrix(blocks_rdd, row_block_count, col_block_count)\n",
    "    \n",
    "def svd(\n",
    "    gram_mat: pyspark.mllib.linalg.distributed.BlockMatrix,\n",
    "    row_block_count: int,\n",
    "    col_block_count: int,\n",
    "    sc\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Perform Singular Value Decomposition (SVD) on a Gramian matrix.\n",
    "    \n",
    "    Args:\n",
    "        gram_mat (BlockMatrix): The input Gramian matrix.\n",
    "        row_block_count (int): Number of rows in each block\n",
    "        col_block_count (int): Number of columns in each block\n",
    "    \n",
    "    Returns:\n",
    "        tuple[BlockMatrix, BlockMatrix, BlockMatrix]: The U, Sigma-inverse, and V^T matrices as BlockMatrices.\n",
    "    \"\"\"\n",
    "    matrix_size: int = 0\n",
    "    tol: float = 0.0\n",
    "    arr: np.ndarray = None\n",
    "    eigval_V: np.ndarray = None\n",
    "    eigvecs: np.ndarray = None\n",
    "    idx: np.ndarray = None\n",
    "    eigvals_inv: np.ndarray = None\n",
    "    block_s_inv: BlockMatrix = None\n",
    "    block_v: BlockMatrix = None\n",
    "    \n",
    "    tol = 1e-15\n",
    "\n",
    "    # Compute and sort eigenvalues and eigenvectors with a numpy Array.\n",
    "    arr = gram_mat.toLocalMatrix().toArray()\n",
    "    eigval_V, eigvecs = np.linalg.eigh(arr)\n",
    "    idx = eigval_V.argsort()[::-1]\n",
    "    eigval_V = eigval_V[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "\n",
    "    # Invert eigenvalues (set small ones to zero) and form a diagonal matrix.\n",
    "    eigvals_inv = np.array([1/val if val > tol else 0 for val in eigval_V])\n",
    "    eigvals_inv = np.diag(eigvals_inv)\n",
    "    \n",
    "    # Convert results to block matrices for Spark and transpose the eigenvectors.\n",
    "    block_s_inv = create_block_matrix_from_numpy(eigvals_inv, int(math.ceil(eigvals_inv.shape[0]/n_workers)), int(math.ceil(eigvals_inv.shape[1]/n_workers)), sc)\n",
    "\n",
    "    block_v = create_block_matrix_from_numpy(eigvecs, int(math.ceil(eigvecs.shape[0]/n_workers)), int(math.ceil(eigvecs.shape[1]/n_workers)), sc)\n",
    "    \n",
    "    return block_v.multiply(block_s_inv).multiply(block_v.transpose())\n",
    "\n",
    "def betacalc(\n",
    "    X: pyspark.mllib.linalg.distributed.BlockMatrix,\n",
    "    Y: pyspark.mllib.linalg.distributed.BlockMatrix,\n",
    "    row_block_count: int,\n",
    "    col_block_count: int,\n",
    "    sc\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculate the beta values for the input matrices X and Y.\n",
    "    \n",
    "    Args:\n",
    "        X (BlockMatrix): The input feature matrix.\n",
    "        Y (BlockMatrix): The target values matrix.\n",
    "        row_block_count (int): Number of rows in each block\n",
    "        col_block_count (int): Number of columns in each block\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The computed beta values.\n",
    "    \"\"\"\n",
    "    XtX: BlockMatrix = None\n",
    "    XtX_inv: BlockMatrix = None\n",
    "    beta: BlockMatrix = None\n",
    "\n",
    "    # Compute gram-matrix\n",
    "    XtX = X.transpose().multiply(X)\n",
    "\n",
    "    # Invert XtX using SVD.\n",
    "    XtX_inv = svd(XtX, row_block_count, col_block_count, sc)\n",
    "\n",
    "    # Compute and return betas\n",
    "    beta = XtX_inv.multiply(X.transpose()).multiply(Y)\n",
    "    return beta\n",
    "\n",
    "def stat_values(\n",
    "    X: pyspark.mllib.linalg.distributed.BlockMatrix,\n",
    "    Y: pyspark.mllib.linalg.distributed.BlockMatrix,\n",
    "    row_block_count: int,\n",
    "    col_block_count: int,\n",
    "    sc\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute comprehensive linear model statistics\n",
    "    Returns formatted string output\n",
    "    \n",
    "    Args:  \n",
    "        X (BlockMatrix): The input feature matrix.\n",
    "        Y (BlockMatrix): The target values matrix.\n",
    "        row_block_count (int): Number of rows in each block\n",
    "        col_block_count (int): Number of columns in each block\n",
    "    Returns:\n",
    "        str: Formatted string containing OLS statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate beta and convert to local\n",
    "    \n",
    "    beta = betacalc(X, Y, row_block_count, col_block_count, sc)\n",
    "    fitted_values = X.multiply(beta)\n",
    "    residuals = Y.subtract(fitted_values)\n",
    "    \n",
    "    fitted_local = fitted_values.toLocalMatrix().toArray()\n",
    "    residuals_local = residuals.toLocalMatrix().toArray()\n",
    "    beta_local = beta.toLocalMatrix().toArray()\n",
    "\n",
    "    # Get local matrices for calculations\n",
    "    X_local = X.toLocalMatrix().toArray()\n",
    "    Y_local = Y.toLocalMatrix().toArray()\n",
    "    \n",
    "    # Basic dimensions\n",
    "    n = X.numRows()\n",
    "    p = X.numCols()\n",
    "    degrees_of_freedom = n - p\n",
    "    \n",
    "    # Residual statistics\n",
    "    rss = np.sum(residuals_local ** 2)  # Residual sum of squares\n",
    "    mse = rss / degrees_of_freedom\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate XtX and its inverse for standard errors\n",
    "    XtX = X.transpose().multiply(X)\n",
    "    XtX_local = XtX.toLocalMatrix().toArray()\n",
    "    XtX_inv = np.linalg.inv(XtX_local)\n",
    "    \n",
    "    # Standard errors and t-values\n",
    "    beta_std_errors = np.sqrt(np.diag(XtX_inv) * mse)\n",
    "    t_values = beta_local.flatten() / beta_std_errors\n",
    "    \n",
    "    # Calculate p-values for t-statistics\n",
    "    p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), degrees_of_freedom))\n",
    "    \n",
    "    # R-squared statistics\n",
    "    y_mean = np.mean(Y_local)\n",
    "    tss = np.sum((Y_local - y_mean) ** 2)  # Total sum of squares\n",
    "    r_squared = 1 - (rss / tss)\n",
    "    adj_r_squared = 1 - (1 - r_squared) * ((n - 1) / degrees_of_freedom)\n",
    "    \n",
    "    # F-statistic\n",
    "    model_ss = tss - rss\n",
    "    f_statistic = (model_ss / (p - 1)) / mse\n",
    "    f_p_value = 1 - stats.f.cdf(f_statistic, p-1, degrees_of_freedom)\n",
    "    \n",
    "    # Residual analysis\n",
    "    residuals_std = residuals_local / np.sqrt(mse)\n",
    "    \n",
    "    # Format output string similar to R's lm()\n",
    "    output = []\n",
    "    output.append(\"Call:\")\n",
    "    output.append(\"Linear Model Fit\\n\")\n",
    "    \n",
    "    output.append(\"Residuals:\")\n",
    "    residual_stats = {\n",
    "        \"Min\": np.min(residuals_local),\n",
    "        \"1Q\": np.percentile(residuals_local, 25),\n",
    "        \"Median\": np.median(residuals_local),\n",
    "        \"3Q\": np.percentile(residuals_local, 75),\n",
    "        \"Max\": np.max(residuals_local)\n",
    "    }\n",
    "    output.append(\"\".join(f\"{k:>8}\" for k in residual_stats.keys()))\n",
    "    output.append(\"\".join(f\"{v:8.4f}\" for v in residual_stats.values()))\n",
    "    output.append(\"\")\n",
    "    \n",
    "    # Coefficients table\n",
    "    output.append(\"Coefficients:\")\n",
    "    output.append(\"              Estimate Std. Error t value Pr(>|t|)\")\n",
    "    for i in range(len(beta_local)):\n",
    "        coef_line = f\"Variable{i:2d} {beta_local[i][0]:9.4f} {beta_std_errors[i]:10.4f} {t_values[i]:7.3f} {p_values[i]:8.4f} \"\n",
    "        # Add significance stars like R\n",
    "        if p_values[i] < 0.001:\n",
    "            coef_line += \"***\"\n",
    "        elif p_values[i] < 0.01:\n",
    "            coef_line += \"** \"\n",
    "        elif p_values[i] < 0.05:\n",
    "            coef_line += \"*  \"\n",
    "        elif p_values[i] < 0.1:\n",
    "            coef_line += \".  \"\n",
    "        output.append(coef_line)\n",
    "    output.append(\"---\")\n",
    "    output.append(\"Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\\n\")\n",
    "    \n",
    "    # Model statistics\n",
    "    output.append(f\"Residual standard error: {np.sqrt(mse):.4f} on {degrees_of_freedom} degrees of freedom\")\n",
    "    output.append(f\"Multiple R-squared: {r_squared:.4f}, Adjusted R-squared: {adj_r_squared:.4f}\")\n",
    "    output.append(f\"F-statistic: {f_statistic:.2f} on {p-1} and {degrees_of_freedom} DF, p-value: {f_p_value:.4e}\\n\")\n",
    "    \n",
    "    # Additional diagnostic information\n",
    "    output.append(\"Additional Statistics:\")\n",
    "    output.append(f\"AIC: {n * np.log(rss/n) + 2*p:.4f}\")\n",
    "    output.append(f\"BIC: {n * np.log(rss/n) + np.log(n)*p:.4f}\")\n",
    "    output.append(f\"RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "def calculate_statistics(\n",
    "    X: pyspark.mllib.linalg.distributed.BlockMatrix,\n",
    "    Y: pyspark.mllib.linalg.distributed.BlockMatrix,\n",
    "    row_block_count: int,\n",
    "    col_block_count: int,\n",
    "    sc\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute comprehensive linear model statistics with statsmodels-style output\n",
    "    \n",
    "    Args:  \n",
    "        X (BlockMatrix): The input feature matrix.\n",
    "        Y (BlockMatrix): The target values matrix.\n",
    "        row_block_count (int): Number of rows in each block\n",
    "        col_block_count (int): Number of columns in each block\n",
    "        sc: Spark context\n",
    "    Returns:\n",
    "        float: elapsed_time for calculating betas\n",
    "        str: Formatted string containing OLS statistics in statsmodels style\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate beta and convert to local\n",
    "    elapsed_time = 0\n",
    "    start_time = time.time()\n",
    "    beta = betacalc(X, Y, row_block_count, col_block_count, sc)\n",
    "    fitted_values = X.multiply(beta)\n",
    "    residuals = Y.subtract(fitted_values)\n",
    "    elapsed_time = elapsed_time + (time.time() - start_time)\n",
    "    \n",
    "    # Convert to local arrays for calculations\n",
    "    fitted_local = fitted_values.toLocalMatrix().toArray()\n",
    "    residuals_local = residuals.toLocalMatrix().toArray()\n",
    "    beta_local = beta.toLocalMatrix().toArray()\n",
    "    X_local = X.toLocalMatrix().toArray()\n",
    "    Y_local = Y.toLocalMatrix().toArray()\n",
    "    \n",
    "    # Basic dimensions\n",
    "    n = X.numRows()\n",
    "    k = X.numCols()  \n",
    "    df_model = k \n",
    "    df_resid = n - k\n",
    "    \n",
    "    # Residual statistics\n",
    "    ssr = np.sum(residuals_local ** 2) \n",
    "    mse = ssr / df_resid\n",
    "    \n",
    "    # Calculate XtX and its inverse for standard errors\n",
    "    XtX = X.transpose().multiply(X)\n",
    "    XtX_local = XtX.toLocalMatrix().toArray()\n",
    "    XtX_inv = np.linalg.inv(XtX_local)\n",
    "    \n",
    "    # Standard errors and t-values\n",
    "    bse = np.sqrt(np.diag(XtX_inv) * mse)\n",
    "    tvalues = beta_local.flatten() / bse\n",
    "    pvalues = 2 * (1 - stats.t.cdf(np.abs(tvalues), df_resid))\n",
    "    \n",
    "    # Uncentered R-squared statistics\n",
    "    tss_uncentered = np.sum(Y_local ** 2) \n",
    "    r2_uncentered = 1 - (ssr / tss_uncentered)\n",
    "    adj_r2_uncentered = 1 - (1 - r2_uncentered) * (n / df_resid)\n",
    "    \n",
    "    # F-test (uncentered)\n",
    "    ess = tss_uncentered - ssr  \n",
    "    fvalue = (ess / df_model) / mse\n",
    "    fpvalue = stats.f.sf(fvalue, df_model, df_resid)\n",
    "    \n",
    "    # Log-likelihood and information criteria\n",
    "    llf = -n/2 * (1 + np.log(2*np.pi) + np.log(ssr/n))\n",
    "    aic = -2 * llf + 2 * k\n",
    "    bic = -2 * llf + np.log(n) * k\n",
    "    \n",
    "    # Additional diagnostics\n",
    "    condition_number = np.sqrt(np.max(np.linalg.eigvals(XtX_local)) / \n",
    "                             np.min(np.linalg.eigvals(XtX_local)))\n",
    "    \n",
    "    # Residual diagnostics with corrected Jarque-Bera test\n",
    "    residuals_standardized = residuals_local.flatten() / np.sqrt(mse)\n",
    "    skew = stats.skew(residuals_standardized)\n",
    "    kurtosis = stats.kurtosis(residuals_standardized, fisher=True)\n",
    "    \n",
    "    # Corrected Jarque-Bera calculation\n",
    "    jb = n/6 * (skew**2 + (kurtosis**2)/4)\n",
    "    jbpv = stats.chi2.sf(jb, df=2)  # Chi-square with 2 df\n",
    "    \n",
    "    # Omnibus test (D'Agostino K^2)\n",
    "    k2, p_omnibus = stats.normaltest(residuals_standardized)\n",
    "    \n",
    "    # Durbin-Watson\n",
    "    dw = np.sum(np.diff(residuals_local.flatten()) ** 2) / ssr\n",
    "    \n",
    "    # Create summary string similar to statsmodels\n",
    "    summary = []\n",
    "    summary.append(\"                            OLS Regression Results                            \")\n",
    "    summary.append(\"===========================================================================\")\n",
    "    summary.append(f\"Dep. Variable:                      y   R-squared (uncentered):     {r2_uncentered:>7.3f}\")\n",
    "    summary.append(f\"Model:                            OLS   Adj. R-squared (uncentered):{adj_r2_uncentered:>7.3f}\")\n",
    "    summary.append(f\"Method:                 Least Squares   F-statistic:                {fvalue:>7.3f}\")\n",
    "    summary.append(f\"Date:                             now   Prob (F-statistic):         {fpvalue:>7.3f}\")\n",
    "    summary.append(f\"Time:                             now   Log-Likelihood:             {llf:>7.3f}\")\n",
    "    summary.append(f\"No. Observations:          {n:>10.0f}   AIC:                        {aic:>7.3f}\")\n",
    "    summary.append(f\"Df Residuals:              {df_resid:>10.0f}   BIC:                        {bic:>7.3f}\")\n",
    "    summary.append(f\"Df Model:                  {df_model:>10.0f}\")\n",
    "    summary.append(\"===========================================================================\")\n",
    "    summary.append(\"                coef    std err          t      P>|t|     [0.025     0.975]\")\n",
    "    summary.append(\"---------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Parameter estimates with confidence intervals\n",
    "    conf_int = np.column_stack((\n",
    "        beta_local.flatten() - stats.t.ppf(0.975, df_resid) * bse,\n",
    "        beta_local.flatten() + stats.t.ppf(0.975, df_resid) * bse\n",
    "    ))\n",
    "    \n",
    "    for i in range(k):\n",
    "        summary.append(f\"x{i:<8.0f} {beta_local[i][0]:>10.3f} {bse[i]:>10.3f} {tvalues[i]:>10.3f} {pvalues[i]:>10.3f} {conf_int[i,0]:>10.3f} {conf_int[i,1]:>10.3f}\")\n",
    "    \n",
    "    summary.append(\"===========================================================================\")\n",
    "    summary.append(f\"Omnibus:                   {k2:>10.3f}   Durbin-Watson:           {dw:>10.3f}\")\n",
    "    summary.append(f\"Prob(Omnibus):             {p_omnibus:>10.3f}   Jarque-Bera (JB):        {jb:>10.3f}\")\n",
    "    summary.append(f\"Skew:                      {skew:>10.3f}   Prob(JB):                {jbpv:>10.3f}\")\n",
    "    summary.append(f\"Kurtosis:                  {kurtosis+3:>10.3f}   Cond. No.                {condition_number:>10.3f}\")\n",
    "    summary.append(\"===========================================================================\")\n",
    "    \n",
    "    return elapsed_time, \"\\n\".join(summary)\n",
    "\n",
    "def run_performance_tests(\n",
    "    n_features_list: list,\n",
    "    n_rows: int,\n",
    "    n_times: int,\n",
    "    n_workers: int,\n",
    "    sc):\n",
    "    \"\"\"\n",
    "    Run performance tests and optionally visualize results\n",
    "    \n",
    "    Args:\n",
    "        n_features_list (list): List of feature sizes to test\n",
    "        n_rows (int): Number of rows in the matrices\n",
    "        n_times (int): Number of test iterations per feature size\n",
    "        n_workers (int): Number of Spark workers\n",
    "        sc: Spark context\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Lists of elapsed_times, avg_elapsed_times, times_per_feature\n",
    "    \"\"\"\n",
    "    elapsed_times = []\n",
    "    avg_elapsed_times = []\n",
    "    times_per_feature = []\n",
    "    \n",
    "    # Run performance tests\n",
    "    for n_features in n_features_list:\n",
    "        row_block_count = math.ceil(n_rows/n_workers)\n",
    "        col_block_count = math.ceil(n_features/n_workers)\n",
    "        matX, matY = create_random_block_matrix_and_vector(\n",
    "            n_rows, n_features, row_block_count, col_block_count, sc\n",
    "        )\n",
    "        elapsed_time = 0\n",
    "        \n",
    "        for a in range(n_times):\n",
    "            elapsed_time, stat = calculate_statistics(\n",
    "                matX, matY, row_block_count, col_block_count, sc\n",
    "            )\n",
    "            print(stat)\n",
    "            \n",
    "        avg_elapsed_time = elapsed_time / n_times\n",
    "        time_per_feature = avg_elapsed_time / n_features\n",
    "        \n",
    "        elapsed_times.append(elapsed_time)\n",
    "        avg_elapsed_times.append(avg_elapsed_time)\n",
    "        times_per_feature.append(time_per_feature)\n",
    "        \n",
    "        print(f\"Features: {n_features}, Gesamtzeit: {elapsed_time:.5f} Sekunden\")\n",
    "        print(f\"Features: {n_features}, durchschnittliche Zeit: {avg_elapsed_time:.5f} Sekunden\")\n",
    "        print(f\"Features: {n_features}, durchschnittliche Zeit pro feature: {time_per_feature:.5f} Sekunden\")\n",
    "    \n",
    "    # Visualization\n",
    "    # Plot 1: Average Time\n",
    "    fig2, ax2 = plt.subplots(figsize=(12, 6))\n",
    "    line2 = ax2.plot(n_features_list, avg_elapsed_times, 'r-', marker='s', \n",
    "                    label='Durchschnittliche Zeit')\n",
    "    ax2.set_xlabel('Anzahl der Features')\n",
    "    ax2.set_ylabel('Durchschnittliche Zeit (Sekunden)', color='r')\n",
    "    plt.title('Laufzeitanalyse: Durchschnittliche Zeit', fontsize=16)\n",
    "    ax2.legend(loc='upper left')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for x, y in zip(n_features_list, avg_elapsed_times):\n",
    "        ax2.annotate(f'{y:.2f}s', (x, y), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', color='r')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 2: Time per Feature\n",
    "    fig3, ax3 = plt.subplots(figsize=(12, 6))\n",
    "    line3 = ax3.plot(n_features_list, times_per_feature, 'g-', marker='s', \n",
    "                    label='Zeit pro Feature')\n",
    "    ax3.set_xlabel('Anzahl der Features')\n",
    "    ax3.set_ylabel('Zeit pro Feature (Sekunden)', color='g')\n",
    "    plt.title('Laufzeitanalyse: Zeit pro Feature', fontsize=16)\n",
    "    ax3.legend(loc='upper right')\n",
    "    ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for x, y in zip(n_features_list, times_per_feature):\n",
    "        ax3.annotate(f'{y:.3f}s', (x, y), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', color='g')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return elapsed_times, avg_elapsed_times, times_per_feature\n",
    "\n",
    "def create_random_block_matrix_and_vector(n_rows: int, n_cols: int, row_block_count: int, col_block_count: int, sc, correlation=0.8):\n",
    "    \"\"\"\n",
    "    Erzeugt zufällige BlockMatrizen für Tests mit korrelierten Daten.\n",
    "    \n",
    "    Argumente:\n",
    "        n_rows (int): Anzahl der Zeilen in der Matrix.\n",
    "        n_cols (int): Anzahl der Spalten in der Matrix.\n",
    "        row_block_count (int): Anzahl der Zeilen in jedem Block.\n",
    "        col_block_count (int): Anzahl der Spalten in jedem Block.\n",
    "        sc: SparkContext für die Parallelisierung.\n",
    "        correlation (float): Korrelationsfaktor zwischen X und Y.\n",
    "        \n",
    "    Rückgabe:\n",
    "        tuple[BlockMatrix, BlockMatrix]: Korrelierte Merkmalsmatrix X und Zielvektor Y als BlockMatrizen,\n",
    "        wobei X die Form (n_rows, n_cols) und Y die Form (n_rows, 1) hat.\n",
    "    \"\"\" \n",
    "    # Generiere zufällige Matrix X, fülle spaltenweise und transponiere dann zurück\n",
    "    X_transposed = np.random.rand(n_cols, n_rows)  # Erstellen als transponierte Matrix\n",
    "    X = X_transposed.T  # Zurück transponieren, um die korrekte Form (n_rows, n_cols) zu erhalten\n",
    "    \n",
    "    # Erzeuge korrelierte Matrix Y basierend auf X und dem angegebenen Korrelationsfaktor\n",
    "    noise = np.random.rand(n_rows, 1) * (1 - correlation)  # Rauschen hinzufügen \n",
    "    Y = correlation * X.mean(axis=1).reshape(-1, 1) + noise\n",
    "\n",
    "    # Konvertiere beide in BlockMatrizen\n",
    "    X_block_matrix = create_block_matrix_from_numpy(X, row_block_count, col_block_count, sc)\n",
    "    Y_block_matrix = create_block_matrix_from_numpy(Y, row_block_count, 1, sc)\n",
    "    \n",
    "    return X_block_matrix, Y_block_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc55de3-d29d-4e58-a048-08af2ad96243",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.$anonfun$parallelize$1(SparkContext.scala:937)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:919)\r\n\tat org.apache.spark.SparkContext.parallelize(SparkContext.scala:936)\r\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromInputStream(JavaRDD.scala:259)\r\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromFile(JavaRDD.scala:239)\r\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:289)\r\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# run Performance Test\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m elapsed_times, avg_times, times_per_feature \u001b[38;5;241m=\u001b[39m \u001b[43mrun_performance_tests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_features_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 400\u001b[0m, in \u001b[0;36mrun_performance_tests\u001b[1;34m(n_features_list, n_rows, n_times, n_workers, sc)\u001b[0m\n\u001b[0;32m    398\u001b[0m row_block_count \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(n_rows\u001b[38;5;241m/\u001b[39mn_workers)\n\u001b[0;32m    399\u001b[0m col_block_count \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(n_features\u001b[38;5;241m/\u001b[39mn_workers)\n\u001b[1;32m--> 400\u001b[0m matX, matY \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_random_block_matrix_and_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_block_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_block_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_times):\n",
      "Cell \u001b[1;32mIn[1], line 484\u001b[0m, in \u001b[0;36mcreate_random_block_matrix_and_vector\u001b[1;34m(n_rows, n_cols, row_block_count, col_block_count, sc, correlation)\u001b[0m\n\u001b[0;32m    481\u001b[0m Y \u001b[38;5;241m=\u001b[39m correlation \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m noise\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Konvertiere beide in BlockMatrizen\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m X_block_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_block_matrix_from_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_block_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_block_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m Y_block_matrix \u001b[38;5;241m=\u001b[39m create_block_matrix_from_numpy(Y, row_block_count, \u001b[38;5;241m1\u001b[39m, sc)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_block_matrix, Y_block_matrix\n",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m, in \u001b[0;36mcreate_block_matrix_from_numpy\u001b[1;34m(np_array, row_block_count, col_block_count, sc)\u001b[0m\n\u001b[0;32m     43\u001b[0m         block_dense_matrix \u001b[38;5;241m=\u001b[39m Matrices\u001b[38;5;241m.\u001b[39mdense(block\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], block\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], block\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     44\u001b[0m         blocks\u001b[38;5;241m.\u001b[39mappend(((i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m row_block_count, j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m col_block_count), block_dense_matrix))\n\u001b[1;32m---> 46\u001b[0m blocks_rdd \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BlockMatrix(blocks_rdd, row_block_count, col_block_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:824\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[1;32m--> 824\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:870\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[1;34m(self, data, serializer, reader_func, server_func)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m         tempFile\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtempFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;66;03m# we eagerly reads the file so we can delete right after.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(tempFile\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\context.py:818\u001b[0m, in \u001b[0;36mSparkContext.parallelize.<locals>.reader_func\u001b[1;34m(temp_filename)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreader_func\u001b[39m(temp_filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadRDDFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumSlices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.SparkContext.$anonfun$parallelize$1(SparkContext.scala:937)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:919)\r\n\tat org.apache.spark.SparkContext.parallelize(SparkContext.scala:936)\r\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromInputStream(JavaRDD.scala:259)\r\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromFile(JavaRDD.scala:239)\r\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:289)\r\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Hyperparameter\n",
    "n_features_list = [5, 8, 10,20,30,40,50,60,100,200,300,400,500,600,700]\n",
    "n_rows = max(n_features_list) * 30\n",
    "n_times = 1\n",
    "n_workers = 4\n",
    "    # Create Sparksession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Optimized-Cluster-Based-SVD-Regression\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"12G\") \\\n",
    "    .config(\"spark.executor.memory\", \"12G\") \\\n",
    "    .config(\"spark.default.parallelism\", n_workers * 2) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", n_workers * 2) \\\n",
    "    .config(\"spark.storage.memoryFraction\", 0.8) \\\n",
    "    .config(\"spark.memory.fraction\", 0.8) \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.task.cpus\", 1) \\\n",
    "    .config(\"spark.rdd.compress\", True) \\\n",
    "    .config(\"spark.broadcast.compress\", True) \\\n",
    "    .config(\"spark.shuffle.compress\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# run Performance Test\n",
    "elapsed_times, avg_times, times_per_feature = run_performance_tests(\n",
    "    n_features_list=n_features_list,\n",
    "    n_rows=n_rows,\n",
    "    n_times=n_times,\n",
    "    n_workers=n_workers,\n",
    "    sc=sc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238b0e9-1313-4b9c-8bd7-eedaa791b2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
