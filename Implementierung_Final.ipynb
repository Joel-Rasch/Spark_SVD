{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7681cd-198c-4a89-887d-afcafed1129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/19 08:05:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "###### from pyspark.sql import SparkSession  \n",
    "import numpy as np  \n",
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session with the configurations  \n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"SVD\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"3\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"4096\") \\\n",
    "    .config(\"spark.cores.max\", \"12\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2537123c-189b-4eea-a5b9-baef1fbac6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b225a81-f8a4-4da1-beb6-8d60743089b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.55839490e-01 -4.91691210e-01  1.07914010e-01 -1.09752762e+00\n",
      " -3.30923572e-01 -1.17692928e+00 -1.13093615e+00  2.83152506e+00\n",
      " -1.10071927e+00 -1.22513413e-01  4.46032266e-01 -2.10802072e-01\n",
      " -1.52952995e+00 -1.41231571e+00  1.77440527e+00 -5.42673093e-01\n",
      " -2.69942196e+00  1.02783801e-01 -8.79170838e-01 -1.04398091e+00\n",
      " -1.42822311e+00  2.33527834e+00  3.03088843e+00 -6.46367791e-01\n",
      " -1.47830272e-01 -2.93175099e-01 -1.16380227e+00  7.72051879e-02\n",
      "  1.02085647e+00 -3.51443067e-01  7.77464038e-01  1.06543317e+00\n",
      "  1.73671000e+00 -1.43123725e-01 -6.62707861e-01 -2.87446994e-01\n",
      " -3.72306562e-02 -1.77799508e+00 -1.64898063e+00 -1.71799251e+00\n",
      "  8.23904062e-03 -1.40547783e-01 -1.09257713e+00  2.38828375e+00\n",
      " -6.90495748e-01 -7.91397390e-01 -1.10719577e+00  1.95416017e+00\n",
      "  2.15168012e+00 -9.21388111e-01  6.12199880e-01  9.68386421e-01\n",
      " -2.10319376e+00 -2.78159066e+00 -2.38329995e+00  3.68915648e+00\n",
      "  3.93165953e-01  4.24978125e-01 -3.65674562e-01 -7.79804654e-01\n",
      "  2.75540848e+00 -1.39467680e+00  3.96710613e-01  3.54037606e+00\n",
      " -5.81489757e-01 -2.13123869e-01 -4.78090119e-01 -1.85959675e+00\n",
      " -2.98258362e-01  1.93432374e+00 -4.39986692e-01 -6.85424115e-04\n",
      " -1.91706559e+00 -3.71473747e+00  1.50717343e+00 -2.30641270e+00\n",
      " -8.37183268e-01 -6.91317595e-01  2.39355185e+00 -2.09269454e+00\n",
      "  1.14625615e+00 -3.17241614e+00  1.00055562e+00  1.60550225e+00\n",
      "  2.55118918e-01  4.20116174e-01  9.71838466e-02  1.29683549e+00\n",
      " -1.04078011e+00  3.94795414e+00 -9.43061281e-01 -3.32721877e-01\n",
      " -4.88549769e-01 -2.87669890e-01 -1.37961320e+00 -7.68235853e-01\n",
      " -3.05687109e+00 -3.60689587e+00 -2.51077932e+00  1.10222786e-01]\n"
     ]
    }
   ],
   "source": [
    "#Final CODE NOT PARALLEL:\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "def generatecorrelatedtestdata(spark, n_samples, n_features, target_r_squared=0.7):\n",
    "    \"\"\"\n",
    "    Generates correlated test data and returns the true coefficients and an RDD.\n",
    "    \"\"\"\n",
    "    # Generate random coefficients\n",
    "    np.random.seed(31)\n",
    "    true_coef = np.random.randn(n_features)\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    X = np.column_stack((X[:,:-1], np.ones(X.shape[0]))) \n",
    "    y_clean = np.dot(X, true_coef)\n",
    "    noise = np.random.randn(n_samples) * np.sqrt(1 - target_r_squared)\n",
    "    y = y_clean + noise\n",
    "    scaling_factor = np.sqrt(target_r_squared / (1 - target_r_squared))\n",
    "    y = y_clean * scaling_factor + noise\n",
    "\n",
    "    # Create an RDD of numpy arrays\n",
    "    rdd = spark.sparkContext.parallelize(X)\n",
    "    return  rdd,y\n",
    "\n",
    "def svd(arr):\n",
    "    \"\"\"\n",
    "    Function to calculate the SVD of a given nxm Matrix.\n",
    "    \"\"\"\n",
    "    # Compute V and singular values\n",
    "    eigval_V, V = np.linalg.eigh(arr.T @ arr)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = eigval_V.argsort()[::-1]\n",
    "    eigval_V = eigval_V[idx]\n",
    "    V = V[:, idx]\n",
    "    \n",
    "    # Compute singular values\n",
    "    singval = np.sqrt(np.abs(eigval_V))\n",
    "    \n",
    "    # Compute U\n",
    "    U = np.zeros((arr.shape[0], min(arr.shape[0], arr.shape[1])))\n",
    "    for i in range(len(singval)):\n",
    "        if singval[i] > 1e-15:  # Avoid division by zero\n",
    "            U[:, i] = (arr @ V[:, i]) / singval[i]\n",
    "    \n",
    "    # Ensure V is returned as V.T\n",
    "    Vt = V.T\n",
    "    \n",
    "    return U, singval, Vt\n",
    "\n",
    "def parallelinvers(U, sigma, V):\n",
    "    # Inverting sigma correctly\n",
    "    sigma_inv = np.diag(1 / sigma)\n",
    "    return V.T @ sigma_inv @ U.T\n",
    "\n",
    "def betacalc(X,Y):\n",
    "    \"\"\"\n",
    "    Calculates the beta values for the input matrix A (X + y).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Compute X^T @ X\n",
    "    XtX = X.T @ X\n",
    "    \n",
    "    # Step 2: SVD decomposition of X^T @ X\n",
    "    U, Sigma, Vt = svd(XtX)\n",
    "    \n",
    "    # Step 3: Calculate the inverse of XtX using SVD components\n",
    "    parinv = parallelinvers(U, Sigma, Vt)\n",
    "    \n",
    "    # Step 4: Compute beta = (X^T X)^(-1) X^T Y\n",
    "    beta = parinv @ X.T @ Y\n",
    "    return beta\n",
    "\n",
    "\n",
    "# Example usage (make sure a SparkSession 'spark' is already created)\n",
    "rdd,y = generatecorrelatedtestdata(spark, 300, 100)\n",
    "X = np.array(rdd.collect())  # Collect RDD into a numpy array\n",
    "beta_values = betacalc(X,y)\n",
    "XtX = X.T @ X\n",
    "U, Sigma, Vt = svd(XtX)\n",
    "print(beta_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "098f17a4-1ed2-4dc6-89aa-6cc560cae1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final CODE PARALLEL\n",
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    " \n",
    "def Gramiam(mat:pyspark.mllib.linalg.distributed.BlockMatrix):\n",
    "    \"\"\"\n",
    "    Compute the inverse of a symmetric positive-definite Gramian matrix using eigenvalue decomposition.\n",
    "\n",
    "    Args:\n",
    "        gram_mat (BlockMatrix): The input Gramian matrix to invert.\n",
    "        chunks (int): Block size for the resulting matrices. Default is 12.\n",
    "\n",
    "    Returns:\n",
    "        BlockMatrix: The inverse of the input Gramian matrix.\n",
    "    \"\"\"\n",
    "    return mat.transpose().multiply(mat)\n",
    "\n",
    "\n",
    "def create_block_matrix_from_numpy(np_array, block_row_size, block_col_size, spark_context):\n",
    "    \"\"\"\n",
    "    Create a BlockMatrix from a NumPy array.\n",
    "    \n",
    "    Args:\n",
    "        np_array (np.ndarray): The NumPy array to be converted to a BlockMatrix.\n",
    "        block_row_size (int): Number of rows in each block.\n",
    "        block_col_size (int): Number of columns in each block.\n",
    "        spark_context (SparkContext): The Spark context used to parallelize the blocks.\n",
    "        \n",
    "    Returns:\n",
    "        BlockMatrix: The resulting BlockMatrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of the numpy array\n",
    "    num_rows, num_cols = np_array.shape\n",
    "    \n",
    "    # List to hold the blocks in the format ((blockRowIndex, blockColIndex), denseMatrix)\n",
    "    blocks = []\n",
    "    \n",
    "    # Divide the array into blocks of block_row_size x block_col_size\n",
    "    for i in range(0, num_rows, block_row_size):\n",
    "        for j in range(0, num_cols, block_col_size):\n",
    "            # Extract the block as a NumPy subarray\n",
    "            block = np_array[i:i + block_row_size, j:j + block_col_size]\n",
    "            \n",
    "            # Convert the block to a DenseMatrix\n",
    "            block_dense_matrix = Matrices.dense(block.shape[0], block.shape[1], block.flatten())\n",
    "            \n",
    "            # Store the block with its block indices (i // block_row_size, j // block_col_size)\n",
    "            blocks.append(((i // block_row_size, j // block_col_size), block_dense_matrix))\n",
    "    \n",
    "    # Parallelize the blocks to create an RDD\n",
    "    blocks_rdd = spark_context.parallelize(blocks)\n",
    "    \n",
    "    # Create and return the BlockMatrix\n",
    "    block_matrix = BlockMatrix(blocks_rdd, block_row_size, block_col_size)\n",
    "    \n",
    "    return block_matrix\n",
    " \n",
    "def svd(gram_mat:pyspark.mllib.linalg.distributed.BlockMatrix,chunks=12):\n",
    "    \"\"\"\n",
    "    Perform Singular Value Decomposition (SVD) on a Gramian matrix.\n",
    "    \n",
    "    Args:\n",
    "        gram_mat (BlockMatrix): The input Gramian matrix.\n",
    "        chunks (int): Block size for the resulting matrices. Default is 12.\n",
    "    \n",
    "    Returns:\n",
    "        tuple[BlockMatrix, BlockMatrix, BlockMatrix]: The U, Sigma-inverse, and V^T matrices as BlockMatrices.\n",
    "    \"\"\"\n",
    "    \n",
    "    arr = gram_mat.toLocalMatrix().toArray()\n",
    "    eigval_V, eigvecs = np.linalg.eigh(arr)\n",
    "        # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = eigval_V.argsort()[::-1]\n",
    "    eigval_V = eigval_V[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    # Compute eigenvalues\n",
    "    print(\"eigval_V\",eigval_V)\n",
    "\n",
    "    print(\"eigvecs\",eigvecs)\n",
    "\n",
    "    tol = 1e-15\n",
    "    eigvals_inv = np.array([1/val if val > tol else 0 for val in eigval_V])\n",
    "    eigvals_inv = np.diag(eigvals_inv)\n",
    "\n",
    "    print(\"eigvals_inv\",eigvals_inv)\n",
    "\n",
    "    block_s_inv = create_block_matrix_from_numpy(eigvals_inv,chunks,chunks,spark.sparkContext)\n",
    "    block_v= create_block_matrix_from_numpy(eigvecs,chunks,chunks,spark.sparkContext)\n",
    "    block_v = block_v.transpose() # needs to be transposed for V as Result not VT\n",
    "\n",
    "    print(\"block_v\",block_v.toLocalMatrix().toArray())\n",
    "    print(\"block_s_inv\",block_s_inv.toLocalMatrix().toArray())\n",
    "    print(\"block_v.transpose()\",block_v.transpose().toLocalMatrix().toArray())\n",
    "\n",
    "    print(\"block_v.multiply(block_s_inv).multiply(block_v.transpose())\",block_v.multiply(block_s_inv).multiply(block_v.transpose()).toLocalMatrix().toArray())\n",
    "\n",
    "    \n",
    "    \n",
    "    return block_v.multiply(block_s_inv).multiply(block_v.transpose())\n",
    "\n",
    "def betacalc(X, Y):\n",
    "    \"\"\"\n",
    "    Calculate the beta values for the input matrices X and Y.\n",
    "    \n",
    "    Args:\n",
    "        X (BlockMatrix): The input feature matrix.\n",
    "        Y (BlockMatrix): The target values matrix.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The computed beta values.\n",
    "    \"\"\"\n",
    "    XtX = X.transpose().multiply(X)\n",
    "    print(\"XtX\",XtX.toLocalMatrix().toArray())\n",
    "    \n",
    "    # Step 4: Compute beta = (X^T X)^(-1) X^T Y\n",
    "    XtX_inv = svd(XtX)\n",
    "    print(\"XtX_inv\",XtX_inv.toLocalMatrix().toArray())\n",
    "\n",
    "\n",
    "    Y = Y.toLocalMatrix().toArray()\n",
    "    X = X.toLocalMatrix().toArray()\n",
    "    XtX_inv = XtX_inv.toLocalMatrix().toArray()\n",
    "\n",
    "    beta = XtX_inv @ X.T @ Y\n",
    "\n",
    "    \n",
    "    return beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8bca2f34-5fee-483e-9733-9892338ce418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XtX [[86. 21. 41.]\n",
      " [21.  9. 16.]\n",
      " [41. 16. 29.]]\n",
      "eigval_V [1.13899537e+02 1.00445226e+01 5.59408056e-02]\n",
      "eigvecs [[-0.85533858 -0.51562151 -0.05030282]\n",
      " [-0.24116714  0.48222298 -0.84219915]\n",
      " [-0.45851317  0.70823404  0.53681489]]\n",
      "eigvals_inv [[8.77966698e-03 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 9.95567475e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.78760386e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_v [[-0.85533858 -0.51562151 -0.05030282]\n",
      " [-0.24116714  0.48222298 -0.84219915]\n",
      " [-0.45851317  0.70823404  0.53681489]]\n",
      "block_s_inv [[8.77966698e-03 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 9.95567475e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.78760386e+01]]\n",
      "block_v.transpose() [[-0.85533858 -0.24116714 -0.45851317]\n",
      " [-0.51562151  0.48222298  0.70823404]\n",
      " [-0.05030282 -0.84219915  0.53681489]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_v.multiply(block_s_inv).multiply(block_v.transpose()) [[ 0.078125  0.734375 -0.515625]\n",
      " [ 0.734375 12.703125 -8.046875]\n",
      " [-0.515625 -8.046875  5.203125]]\n",
      "XtX_inv [[ 0.078125  0.734375 -0.515625]\n",
      " [ 0.734375 12.703125 -8.046875]\n",
      " [-0.515625 -8.046875  5.203125]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.5],\n",
       "       [ 2.5],\n",
       "       [-1.5]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BeispielDaten\n",
    "\n",
    "# Parallel Data\n",
    "sc = spark.sparkContext\n",
    "\n",
    "dm1 = Matrices.dense(3, 4, [1, 2, 3, 6, 2, 4,7,1,2])\n",
    "\n",
    "blocks1 = sc.parallelize([((0, 0), dm1)])\n",
    "\n",
    "mat1 = BlockMatrix(blocks1, 3, 3)\n",
    "\n",
    "Y =  Matrices.dense(3, 1, [1, 2, 3])\n",
    "blocksy = sc.parallelize([((0, 0), Y)])\n",
    "matY = BlockMatrix(blocksy,3,1)\n",
    "\n",
    "betacalc(mat1.transpose(),matY)# Nutze mat1 transpose, da mat1 falsch formatiert ist. Bitte MAtrix korrekt formatieren\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
